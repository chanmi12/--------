import os, re, time, hashlib, random, shutil, math
from typing import List, Dict, Any, Tuple
import concurrent.futures as _f

import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import requests
import numpy as np

# Streamlit í˜ì´ì§€ ì„¤ì •(ìµœìƒë‹¨)
st.set_page_config(page_title="ë¡œì»¬ RAG ì±—ë´‡ (Ollama + Chroma)", page_icon="ğŸ§ ", layout="wide")

# LangChain & Chroma/Ollama
from langchain_chroma import Chroma
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ì˜µì…˜: BM25 ìˆìœ¼ë©´ ì‚¬ìš©
try:
    from rank_bm25 import BM25Okapi
    _BM25_AVAILABLE = True
except Exception:
    _BM25_AVAILABLE = False

# ===================== í™˜ê²½ ë¡œë“œ =====================
load_dotenv()

EMB_BACKEND        = os.getenv("EMB_BACKEND", "ollama").lower()
OLLAMA_BASE_URL    = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434").replace("/v1","")
OLLAMA_GEN_MODEL   = os.getenv("OLLAMA_GEN_MODEL", "llama3.1:8b-instruct-q4_0")
OLLAMA_EMBED_MODEL = os.getenv("OLLAMA_EMBED_MODEL", "mxbai-embed-large")

CSV_DEFAULT        = os.getenv("CSV_DEFAULT", "test.csv")
PERSIST_DIR        = os.getenv("PERSIST_DIR", "./chroma_creation")
COLLECTION_NAME    = os.getenv("COLLECTION_NAME", "")

CHUNK_CHARS        = int(os.getenv("CHUNK_CHARS", "1600"))
CHUNK_OVERLAP      = int(os.getenv("CHUNK_OVERLAP", "150"))
MAX_EMBED_CHARS    = int(os.getenv("MAX_EMBED_CHARS", "4000"))
FRESH              = os.getenv("FRESH", "true").lower() in ("1","true","yes","y")

FORCE_FETCH_DOMAINS = {"creation.kr"}
MIN_CONTENT_LEN = 50

# ===================== Fast ì˜µì…˜ í”Œë˜ê·¸ ê¸°ë³¸ê°’ =====================
FAST_SOURCE_SELECT_DEFAULT = True  # ì¶œì²˜ì„ ì •ì—ì„œ LLM ì—”í…Œì¼ë¨¼íŠ¸ ìƒëµ(í‚¤ì›Œë“œ ì í•©ë„ ê¸°ë°˜)

# ===================== ê³µìš© ìºì‹œ/ë¦¬ì†ŒìŠ¤ =====================
@st.cache_resource(show_spinner=False)
def _get_emb() -> OllamaEmbeddings:
    return OllamaEmbeddings(model=OLLAMA_EMBED_MODEL, base_url=OLLAMA_BASE_URL)

@st.cache_resource(show_spinner=False)
def _get_llm(temp: float = 0.2, num_predict: int = 200) -> ChatOllama:
    return ChatOllama(
        model=OLLAMA_GEN_MODEL,
        base_url=OLLAMA_BASE_URL,
        temperature=temp,
        model_kwargs={
            "num_predict": num_predict,
            "keep_alive": "10m",
            "num_thread": 0,
        },
    )

@st.cache_resource(show_spinner=False)
def _get_llm_zero() -> ChatOllama:
    return ChatOllama(model=OLLAMA_GEN_MODEL, base_url=OLLAMA_BASE_URL, temperature=0.0,
                      model_kwargs={"keep_alive": "10m", "num_thread": 0})

@st.cache_resource(show_spinner=False)
def _warm_llm_once() -> bool:
    try:
        _ = _get_llm_zero().invoke([{"role":"user","content":"ping"}])
    except Exception:
        pass
    return True

# ===================== ìœ í‹¸ =====================
@st.cache_data(show_spinner=False)
def _cached_read_csv(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path, dtype=str).fillna("")
    return df

def df_fingerprint(df: pd.DataFrame) -> str:
    parts = [(row.get("title","") or "") + (row.get("content","") or "") for _, row in df.iterrows()]
    return hashlib.sha1("|".join(parts).encode("utf-8")).hexdigest()

def persist_path(persist_dir: str, fp: str) -> Tuple[str, str]:
    d = os.path.join(persist_dir, f"chroma_{fp[:12]}")
    return d, f"creation_{fp[:12]}"

def load_csv(csv_path: str) -> pd.DataFrame:
    csv_path = csv_path if os.path.isabs(csv_path) else os.path.join(os.getcwd(), csv_path)
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
    df = _cached_read_csv(csv_path)
    need_cols = {"url","title","content","references","further_refs"}
    missing = need_cols - set(df.columns)
    if missing:
        raise ValueError(f"CSVì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing}")
    return df

def need_force_fetch(url: str) -> bool:
    try:
        host = re.sub(r"^https?://", "", url).split("/")[0]
        return any(host.endswith(dom) for dom in FORCE_FETCH_DOMAINS)
    except Exception:
        return False

def _smart_select_main(soup: BeautifulSoup):
    for css in [
        "article",".fr-view",".rd-content",".board_view",".boardView",".content",
        "#content","#article","#view",".editor_content",".xe_content",".se-component",
    ]:
        node = soup.select_one(css)
        if node and node.get_text(strip=True):
            return node
    return soup.body or soup

@st.cache_data(show_spinner=False)
def fetch_url_text(url: str, timeout: int = 12, max_len: int = 25000, retries: int = 2) -> str:
    if not url or not re.match(r"^https?://", url):
        return ""
    for attempt in range(retries + 1):
        try:
            r = requests.get(url, headers={"User-Agent":"Mozilla/5.0 (CreationKR/1.0)"}, timeout=timeout)
            r.raise_for_status()
            if not r.encoding or r.encoding.lower() in ("iso-8859-1","ascii"):
                r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "htmlparser" if "htmlparser" in str(BeautifulSoup).lower() else "html.parser")
            for tag in soup(["script","style","nav","footer","header","aside","form"]):
                tag.decompose()
            main = _smart_select_main(soup)
            text = (main or soup).get_text("\n")
            lines = [re.sub(r"\s+", " ", ln).strip() for ln in text.splitlines()]
            text = "\n".join([ln for ln in lines if ln])[:max_len]
            if len(text) < 150 and attempt < retries:
                time.sleep(0.2)
                continue
            return text
        except Exception:
            time.sleep(0.2)
    return ""

def safe_truncate(s: str, max_chars: int) -> str:
    if len(s) <= max_chars:
        return s
    cut = s[:max_chars]
    last = max(cut.rfind("\n"), cut.rfind(". "), cut.rfind("ã€‚"), cut.rfind("! "), cut.rfind("? "))
    if last >= max_chars * 0.7:
        return cut[:last].rstrip()
    return cut.rstrip()

@st.cache_data(show_spinner=False)
def _split_base_text(base_text: str) -> List[str]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_CHARS,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", " ", ""],
    )
    return splitter.split_text(base_text)

def docs_from_df(df: pd.DataFrame, do_network_enrich: bool = False) -> List[Document]:
    docs: List[Document] = []
    for ridx, row in df.iterrows():
        title = (row.get("title") or "").strip()
        content = (row.get("content") or "").strip()
        url = (row.get("url") or "").strip()
        references_raw = (row.get("references") or "").strip()
        further_refs_raw = (row.get("further_refs") or "").strip()

        if do_network_enrich and url and (len(content) < MIN_CONTENT_LEN or need_force_fetch(url)):
            fetched = fetch_url_text(url)
            if fetched:
                content = (content + "\n\n" + fetched).strip()

        if not title and not content:
            continue

        base_text = f"{title}\n\n{content}".strip()
        chunks = _split_base_text(base_text)

        final_chunks = []
        for ch in chunks:
            if len(ch) <= MAX_EMBED_CHARS:
                final_chunks.append(ch)
            else:
                start = 0
                while start < len(ch):
                    piece = safe_truncate(ch[start:start + MAX_EMBED_CHARS + 500], MAX_EMBED_CHARS)
                    if not piece:
                        break
                    final_chunks.append(piece)
                    start += len(piece)

        for cidx, chunk in enumerate(final_chunks):
            docs.append(Document(
                page_content=chunk,
                metadata={
                    "title": title,
                    "url": url,
                    "references_raw": references_raw,
                    "further_refs_raw": further_refs_raw,
                    "row_id": str(ridx),
                    "chunk_id": f"{ridx}-{cidx}",
                }
            ))
    return docs

def _collection_is_empty(store: Chroma) -> bool:
    try:
        cnt = store._collection.count()  # type: ignore[attr-defined]
        return (cnt or 0) == 0
    except Exception:
        try:
            got = store._collection.get(limit=1)  # type: ignore[attr-defined]
            return not bool(got.get("ids"))
        except Exception:
            try:
                _ = store.similarity_search("ping", k=1)
                return False
            except Exception:
                return True

@st.cache_resource(show_spinner=False)
def _open_store(persist_directory: str, collection_name: str, _emb: OllamaEmbeddings) -> Chroma:
    # Streamlit ìºì‹œ í•´ì‹œ ì˜¤ë¥˜ íšŒí”¼: ì–¸ë”ìŠ¤ì½”ì–´ ì ‘ë‘ íŒŒë¼ë¯¸í„°ëŠ” í•´ì‹œì—ì„œ ì œì™¸ë¨
    return Chroma(persist_directory=persist_directory, collection_name=collection_name, embedding_function=_emb)

@st.cache_data(show_spinner=False)
def _store_id(persist_dir: str, coll: str) -> str:
    return hashlib.sha1(f"{persist_dir}::{coll}".encode("utf-8")).hexdigest()

def build_or_load_store(csv_path: str, persist_dir: str, collection_name_env: str, fresh: bool) -> Tuple[Chroma, str, str]:
    df = load_csv(csv_path)
    fp = df_fingerprint(df)
    d_auto, cname_auto = persist_path(persist_dir, fp)

    d = d_auto
    cname = (collection_name_env or "").strip() or cname_auto

    if fresh and os.path.isdir(d):
        shutil.rmtree(d)
    os.makedirs(d, exist_ok=True)

    emb = _get_emb()

    # ì„ë² ë”© ëª¨ë¸ í—¬ìŠ¤ì²´í¬
    try:
        _ = emb.embed_query("health check")
    except Exception as e:
        msg = str(e)
        hint = ""
        if "model" in msg.lower() and "not found" in msg.lower():
            hint = f"\nğŸ’¡ í•´ê²°: `ollama pull {OLLAMA_EMBED_MODEL}` ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”."
        raise RuntimeError(f"ì„ë² ë”© ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}{hint}")

    store = _open_store(d, cname, emb)

    need_index = fresh or _collection_is_empty(store)
    if need_index:
        with st.spinner("ì¸ë±ìŠ¤ë¥¼ ìƒì„±/ê°±ì‹  ì¤‘..."):
            docs = docs_from_df(df, do_network_enrich=False)
            add_with_backoff(store, docs, batch_size=32)

    return store, d, cname

def add_with_backoff(store: Chroma, docs: List[Document], batch_size=32, max_retries=8):
    n = len(docs); i = 0
    while i < n:
        j = min(i + batch_size, n)
        batch = docs[i:j]
        attempt = 0
        while True:
            try:
                store.add_documents(batch)
                break
            except Exception as e:
                wait = min(2 ** attempt, 10) + random.uniform(0, 0.2)
                st.warning(f"[index] add_documents error: {e} (ì¬ì‹œë„ {wait:.1f}s)")
                time.sleep(wait)
                attempt += 1
                if attempt >= max_retries:
                    raise
        i = j

# ===================== í”„ë¡¬í”„íŠ¸ & RAG ì²´ì¸ =====================
SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í•œêµ­ì–´ ì—°êµ¬ ë³´ì¡°ìì…ë‹ˆë‹¤. "
    "ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì§€ì‹ì˜ ê·¼ê±°ë¡œ ì‚¼ë˜, ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ì§€ ë§ê³  ë°˜ë“œì‹œ ì¬êµ¬ì„±í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•˜ì„¸ìš”. "
    "í‘œí˜„ ë°©ì‹, ë¬¸ì¥ êµ¬ì¡°, ì„¤ëª… íë¦„ì„ ë°”ê¿”ì„œ ìƒˆë¡­ê²Œ ì„œìˆ í•´ì•¼ í•©ë‹ˆë‹¤. "
    "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì‚¬ì‹¤ì€ ë§í•˜ì§€ ë§ê³ , ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”."
)


USER_Q_TEMPLATE = (
    "ì§ˆë¬¸: {question}\n\n"
    "[ì»¨í…ìŠ¤íŠ¸]\n{context}\n\n"
    "ìš”êµ¬ì‚¬í•­:\n"
    # "- ìœ„ ì»¨í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ 'ì •ë‹µ'ê³¼ 'ê´€ë ¨ ì„¤ëª…'ì„ ì‘ì„±í•˜ì„¸ìš”.\n"
    # "- ì •ë‹µì€ ì»¨í…ìŠ¤íŠ¸ì— ëª…ì‹œëœ ì •ë³´ë§Œ ê·¼ê±°ë¡œ ì‘ì„±í•˜ì„¸ìš”."
    "- ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ë¬¸ì„œì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”."
    "- ë³¸ë¬¸ì—ëŠ” ë§í¬/ì¶œì²˜ë¥¼ ì“°ì§€ ë§ˆì„¸ìš”.\n"
    "- ì˜ëª»ëœ ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.\n"
    "- ë§ì¶¤ë²•ì€ ì§€í‚¤ì„¸ìš”.\n"
)


# ===================== ì»¨í…ìŠ¤íŠ¸ ì••ì¶• =====================
def _sent_tokenize(text: str) -> List[str]:
    # ê°„ë‹¨ ë¬¸ì¥ ë¶„í• (ì˜/í•œ í˜¼í•©ìš©)
    parts = re.split(r'(?<=[\.\?\!ã€‚])\s+|\n+', text)
    return [p.strip() for p in parts if p and len(p.strip()) >= 5]

def _keyword_score(sent: str, terms: List[str]) -> float:
    s = sent.lower()
    return sum(1 for t in terms if t in s) / max(1, len(terms) or 1)

def _compress_context(docs_serialized: List[Tuple[str, Dict[str, Any]]], question: str, max_chars: int = 3500) -> str:
    terms = [t for t in re.split(r"[\W_]+", question.lower()) if len(t) >= 2]
    scored_sents = []
    for page_content, meta in docs_serialized:
        title = meta.get("title") or ""
        title_boost = 0.2 * _keyword_score(title, terms)
        for s in _sent_tokenize(page_content):
            scored_sents.append((0.8 * _keyword_score(s, terms) + title_boost, title, s))
    scored_sents.sort(key=lambda x: x[0], reverse=True)
    out = []
    used = 0
    for _, title, s in scored_sents:
        block = f"### {title}\n{s}"
        if used + len(block) + 2 > max_chars:
            break
        out.append(block); used += len(block) + 2
        if used > max_chars * 0.9:
            break
    if not out:
        # fallback: ì›ë³¸ ì•ë¶€ë¶„ ì••ì¶• ì—†ì´
        parts = []
        for page_content, meta in docs_serialized:
            head = meta.get("title") or "(ì œëª© ì—†ìŒ)"
            parts.append(f"### {head}\n{page_content}")
        return ("\n\n".join(parts))[:max_chars]
    return "\n\n".join(out)

# -------------------- ì¼ë°˜í™” í‚¤ì›Œë“œ/ìŠ¤ì½”ì–´ ìœ í‹¸ --------------------
_STOPWORDS = set("""
ì€ ëŠ” ì´ ê°€ ì„ ë¥¼ ì— ì˜ ì™€ ê³¼ ë„ ë¡œ ìœ¼ë¡œ ì—ì„œ í•œ í•˜ê³  ì´ë‚˜ ë‚˜ ë˜ëŠ” í˜¹ì€ ê·¸ë¦¬ê³  ê·¸ëŸ¬ë‚˜ ê·¸ë˜ì„œ
the a an and or of to in on for with from by as at is are was were be been being this that those these it its if then
""".split())

def extract_keywords(text: str, min_len: int = 2) -> List[str]:
    toks = [t.lower() for t in re.split(r"[\W_]+", text) if t]
    toks = [t for t in toks if len(t) >= min_len and t not in _STOPWORDS]
    if not toks:
        return []
    vals, cnts = np.unique(toks, return_counts=True)
    pairs = sorted(zip(vals, cnts), key=lambda x: x[1], reverse=True)
    return [w for w, _ in pairs[:20]]

def _normalize_q(q: str) -> str:
    q = q.strip().lower()
    q = re.sub(r"\s+", " ", q)
    return q

# ===================== ê²€ìƒ‰ & ì¬ë­í‚¹(ì •í™•ë„ í•µì‹¬) =====================
@st.cache_data(show_spinner=False)
def _load_all_chunks_cached(persist_dir_used: str, collection_used: str) -> Tuple[List[List[str]], List[str], List[Dict[str, Any]]]:
    emb = _get_emb()
    store = _open_store(persist_dir_used, collection_used, emb)
    docs = store._collection.get(include=["documents", "metadatas"])  # type: ignore[attr-defined]
    texts = docs.get("documents", []) or []
    metas = docs.get("metadatas", []) or []
    tokenized = [t.split() for t in texts]
    return tokenized, texts, metas

def _current_store_id() -> str:
    persist = st.session_state.get("persist_dir_used", PERSIST_DIR)
    coll = st.session_state.get("collection_used", COLLECTION_NAME or "(auto)")
    return hashlib.sha1(f"{persist}::{coll}".encode("utf-8")).hexdigest()

def _ensure_bm25_index(store: Chroma):
    if not _BM25_AVAILABLE:
        return None
    sid = _current_store_id()
    if "bm25" in st.session_state and st.session_state.get("bm25_store_id") == sid:
        return st.session_state.bm25
    tokenized, texts, metas = _load_all_chunks_cached(
        st.session_state.get("persist_dir_used", PERSIST_DIR),
        st.session_state.get("collection_used", COLLECTION_NAME or "(auto)")
    )
    if not texts:
        return None
    bm25 = BM25Okapi(tokenized)
    st.session_state.bm25 = bm25
    st.session_state.bm25_texts = texts
    st.session_state.bm25_metas = metas
    st.session_state.bm25_store_id = sid
    return bm25

def _dense_search_with_scores(store: Chroma, query: str, k: int) -> List[Tuple[Document, float]]:
    try:
        pairs = store.similarity_search_with_relevance_scores(query, k=k)
        return [(doc, float(score if score is not None else 0.0)) for doc, score in pairs]
    except Exception:
        docs = store.similarity_search(query, k=k)
        return [(d, 0.0) for d in docs]

@st.cache_data(show_spinner=False)
def _hyde_query_cached(q: str) -> str:
    qn = _normalize_q(q)
    llm_tmp = _get_llm_zero()
    prompt = "ì§ˆë¬¸ì— ëŒ€í•œ ê°„ê²°í•œ ê°€ì„¤ì  ë‹µë³€ì„ í•œ ë‹¨ë½ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\nì§ˆë¬¸: " + qn
    resp = llm_tmp.invoke([{"role":"user","content":prompt}])
    return (getattr(resp, "content", str(resp)) or "").strip()

@st.cache_data(show_spinner=False)
def _paraphrases_cached(q: str, n: int = 2) -> List[str]:
    qn = _normalize_q(q)
    llm0 = _get_llm_zero()
    prompt = (
        "ì•„ë˜ ì§ˆë¬¸ì„ ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì˜ ì§ˆì˜ 2ê°œë¡œ ì§§ê²Œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆí•˜ì„¸ìš”.\n"
        "ì¶œë ¥ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„ëœ 2ì¤„, ë¶ˆë¦¿ ê¸ˆì§€.\nì§ˆë¬¸: " + qn
    )
    rsp = llm0.invoke([{"role":"user","content":prompt}])
    raw = (getattr(rsp, "content", str(rsp)) or "").strip()
    out = [ln.strip("-â€¢ ").strip() for ln in raw.splitlines() if ln.strip()]
    return out[:n]

def _keyword_bonus(text: str, terms: List[str]) -> float:
    s = text.lower()
    return sum(1 for t in terms if t in s) / max(1, len(terms) or 1)

def _rrf_fusion(rank_lists: List[List[Tuple[str, Tuple[Document,float]]]], k: int, K: int = 60) -> List[Tuple[Document, float]]:
    # rank_lists: [ [(key,(doc,score)), ...], ... ]
    agg: Dict[str, Tuple[Document, float]] = {}
    for ranks in rank_lists:
        for r, (key, (doc, _)) in enumerate(ranks, start=1):
            prev = agg.get(key, (doc, 0.0))
            agg[key] = (doc, prev[1] + 1.0 / (K + r))
    items = list(agg.items())
    items.sort(key=lambda x: x[1][1], reverse=True)
    return [(doc, score) for _, (doc, score) in items[:k]]

def _rankify(pairs: List[Tuple[Document, float]], terms: List[str], weight_factor: float = 1.2) -> List[Tuple[str, Tuple[Document, float]]]:
    # ì¶”ê°€ëœ ê°€ì¤‘ì¹˜ë¡œ ì •ë ¬
    scored = []
    for doc, s in pairs:
        meta = doc.metadata or {}
        title = meta.get("title", "")
        content = doc.page_content
        score = s + weight_factor * _keyword_bonus(title + content, terms)
        scored.append((doc, score))
    scored.sort(key=lambda x: x[1], reverse=True)
    out = []
    for i, (doc, sc) in enumerate(scored, start=1):
        key = f"{meta.get('row_id', '?')}-{meta.get('chunk_id', '?')}-{i}"
        out.append((key, (doc, sc)))
    return out

    
def _mmr_diversify(docs: List[Document], top_k: int, lamb: float = 0.7) -> List[Document]:
    # í† í° êµì§‘í•© ê·¼ì‚¬ ìœ ì‚¬ë„(Jaccard-like)
    def _tokset(text: str):
        return set([t for t in re.split(r"[\W_]+", text.lower()) if len(t) >= 3])
    cand = [(doc, _tokset(doc.page_content)) for doc in docs]
    selected: List[Tuple[Document, set]] = []
    while cand and len(selected) < top_k:
        best_i, best_score = 0, -1e9
        for i, (doc_i, set_i) in enumerate(cand):
            rel = 1.0  # ì´ë¯¸ ì¬ë­í¬ëœ ë¦¬ìŠ¤íŠ¸ë¼ ê°€ì¤‘ì¹˜ ë¹„ìŠ·í•˜ê²Œ ì·¨ê¸‰
            div = 0.0
            if selected:
                div = max(len(set_i & sset)/max(1,len(set_i|sset)) for _, sset in selected)
            score = lamb*rel - (1-lamb)*div
            if score > best_score:
                best_score, best_i = score, i
        selected.append(cand.pop(best_i))
    return [d for d,_ in selected]

def _neighbor_docs_serialized(chunk_id: str, texts: List[str], metas: List[Dict[str, Any]], window: int = 1) -> List[Tuple[str, Dict[str, Any]]]:
    try:
        ridx_str, cidx_str = chunk_id.split("-")
        ridx = int(ridx_str); cidx = int(cidx_str)
    except Exception:
        return []
    out: List[Tuple[str, Dict[str, Any]]] = []
    for meta, text in zip(metas, texts):
        if str(meta.get("row_id")) != str(ridx):
            continue
        try:
            cid = meta.get("chunk_id", "0-0")
            _, c = cid.split("-")
            if abs(int(c) - cidx) <= window and int(c) != cidx:
                out.append((text, meta))
        except Exception:
            continue
    return out

def hybrid_retrieve_with_scores(store: Chroma, question: str, k: int, mode: str,
                               dense_weight: float = 0.6, window: int = 1,
                               use_multiquery: bool = True) -> List[Document]:
    terms = extract_keywords(question)
    queries = [question]

    if mode in ("HyDE+Hybrid", "Hybrid") and use_multiquery:
        try:
            hyp = _hyde_query_cached(question)
            queries.append(f"{question}\n{hyp}")
        except Exception:
            pass
        try:
            for pq in _paraphrases_cached(question, n=2):
                if pq and pq not in queries:
                    queries.append(pq)
        except Exception:
            pass

    # Dense/BM25 ê°ê° ìƒìœ„ í›„ë³´ë“¤ì„ RRFë¡œ ê²°í•©
    dense_ranklists = []
    bm25_ranklists  = []
    for q in queries:
        dense_pairs = _dense_search_with_scores(store, q, k=max(k*2, k))
        dense_ranklists.append(_rankify(dense_pairs, terms))
        if _BM25_AVAILABLE and mode in ("Lexical", "Hybrid", "HyDE+Hybrid"):
            bm25 = _ensure_bm25_index(store)
            if bm25 is not None:
                tokenized_q = q.split()
                scores = bm25.get_scores(tokenized_q)
                idx_sorted = np.argsort(scores)[::-1][:max(k*2, k)]
                texts = st.session_state.bm25_texts
                metas = st.session_state.bm25_metas
                bm25_pairs = []
                for i in idx_sorted:
                    meta = metas[int(i)]
                    text = texts[int(i)]
                    bm25_pairs.append((Document(page_content=text, metadata=meta), float(scores[int(i)])))
                bm25_ranklists.append(_rankify(bm25_pairs, terms))

    fused_docs: List[Document] = []
    # RRF ê²°í•©
    ranklists = []
    if dense_ranklists: ranklists += dense_ranklists
    if bm25_ranklists:  ranklists += bm25_ranklists
    if ranklists:
        fused = _rrf_fusion(ranklists, k=max(k*4, k))
        fused_docs = [doc for doc, _ in fused]
    else:
        # Dense fallback
        fused_docs = [d for d,_ in _dense_search_with_scores(store, question, k=max(k*3, k))]

    # MMR ë‹¤ì–‘í™”
    diversified = _mmr_diversify(fused_docs, top_k=k)

    # ì´ì›ƒ í™•ì¥
    texts = st.session_state.get("bm25_texts", [])
    metas = st.session_state.get("bm25_metas", [])
    out: List[Document] = []
    seen = set()
    for doc in diversified:
        cid = doc.metadata.get("chunk_id","?")
        if cid not in seen:
            out.append(doc); seen.add(cid)
        if texts and metas and window > 0:
            for page_text, meta in _neighbor_docs_serialized(cid, texts, metas, window=window):
                ncid = meta.get("chunk_id","?")
                if ncid not in seen:
                    out.append(Document(page_content=page_text, metadata=meta)); seen.add(ncid)
    return out[:max(k+window*2, k)]

# -------------------- ì¶œì²˜ ì„ íƒ & ë¯¸ë‹ˆ ê²€ì¦ --------------------
@st.cache_data(show_spinner=False)
def _extract_facts_from_answer_cached(answer_text: str) -> List[str]:
    llm0 = _get_llm_zero()
    prompt = (
        "ë‹¤ìŒ ë‹µë³€ì—ì„œ ê²€ì¦ì´ í•„ìš”í•œ í•µì‹¬ ì‚¬ì‹¤ë§Œ 3~6ê°œ bulletë¡œ ë§¤ìš° ì§§ê²Œ ì¶”ì¶œí•˜ì„¸ìš”.\n"
        "í˜•ì‹: ê° ì¤„ í•˜ë‚˜ì˜ ì‚¬ì‹¤. ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±°. ê³ ìœ ëª…ì‚¬/ìˆ˜ëŸ‰/ê´€ê³„ë¥¼ ì‚´ë¦¬ë˜ 80ì ì´ë‚´.\n\n"
        f"{answer_text}\n"
    )
    rsp = llm0.invoke([{"role":"user","content":prompt}])
    raw = (getattr(rsp, "content", str(rsp)) or "")
    facts = []
    for line in raw.splitlines():
        line = line.strip("-â€¢ \t").strip()
        if line:
            facts.append(line)
    return facts[:6]

@st.cache_data(show_spinner=False)
def _entailment_score_cached(fact: str, candidate_text: str) -> float:
    llm0 = _get_llm_zero()
    judge = (
        "ë¬¸ì„œê°€ ì£¼ì–´ì§„ ì‚¬ì‹¤ì„ ë’·ë°›ì¹¨í•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.\n"
        "- ì¶œë ¥ì€ 0.0~1.0 ì‚¬ì´ ì†Œìˆ˜ì  í•˜ë‚˜ë§Œ(ì„¤ëª… ê¸ˆì§€)\n"
        "- 1.0=ê°•í•˜ê²Œ ë’·ë°›ì¹¨, 0.5=ì• ë§¤, 0.0=ë¶€ì •/ê´€ë ¨ì—†ìŒ\n\n"
        f"[ì‚¬ì‹¤]\n{fact}\n\n"
        f"[ë¬¸ì„œ]\n{candidate_text[:2800]}\n"
    )
    r = llm0.invoke([{"role":"user","content":judge}])
    s = (getattr(r, "content", str(r)) or "").strip()
    try:
        v = float(re.findall(r"[01](?:\.\d+)?", s)[0])
        return max(0.0, min(1.0, v))
    except Exception:
        return 0.0

def _select_primary_source_fast(question: str, candidates_serialized: List[Tuple[str, Dict[str, Any]]], top_n: int = 6) -> Dict[str, Any]:
    terms = [t for t in re.split(r"[\W_]+", question.lower()) if len(t) >= 2]
    best, best_s = None, -1.0
    for (page_content, meta) in candidates_serialized[:max(1, top_n)]:
        t = (meta.get("title") or "") + " " + page_content
        sc = _keyword_bonus(t, terms)
        if sc > best_s:
            best_s, best = sc, meta
    return best or (candidates_serialized[0][1] if candidates_serialized else {})

def _primary_source_line(meta: Dict[str, Any]) -> str:
    if not meta:
        return "- (ì¶œì²˜ ì—†ìŒ)"
    title = (meta.get("title") or "").strip() or "(ì œëª© ì—†ìŒ)"
    url = (meta.get("url") or "").strip()
    if url:
        return f"{title} | {url}"
    return f"{title}"

def _dynamic_num_predict(question: str, ctx_chars: int) -> int:
    base = 160 if len(question) > 60 or ctx_chars > 2500 else 120
    return max(96, min(200, base))

def rag_answer(store: Chroma, question: str, k: int = 5,
               mode: str = "Hybrid", dense_weight: float = 0.6, neighbor_window: int = 1,
               fast_mode: bool = True, use_multiquery: bool = True, strict_verify: bool = False
               ) -> Tuple[str, List[Document]]:
    # 1) ê²€ìƒ‰ + ì¬ë­í¬ + ë‹¤ì–‘í™”
    fetched_docs = hybrid_retrieve_with_scores(
        store, question, k=k, mode=mode, dense_weight=dense_weight, window=neighbor_window,
        use_multiquery=use_multiquery and not fast_mode  # Fastì¼ ë•Œ ë©€í‹°ì¿¼ë¦¬ offë¡œ ì§€ì—° ìµœì†Œí™”
    )

    total_chars = sum(len(d.page_content) for d in fetched_docs)
    if len(fetched_docs) == 0 or total_chars < 400:
        return (
            "ì£„ì†¡í•´ìš”. ì œê³µëœ ë¬¸ì„œë“¤ë§Œìœ¼ë¡œëŠ” ì§ˆë¬¸ì— ë‹µí•˜ê¸°ì— ì¶©ë¶„í•œ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. "
            "ì§ˆë¬¸ì„ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ CSVì— ê´€ë ¨ ìë£Œë¥¼ ì¶”ê°€í•´ ì£¼ì„¸ìš”.",
            fetched_docs,
        )

    # 2) ì»¨í…ìŠ¤íŠ¸ ì••ì¶•(ì •í™•ë„ ìœ ì§€ + í† í° ì ˆì•½)
    docs_serialized: List[Tuple[str, Dict[str, Any]]] = [
        (d.page_content, dict(d.metadata)) for d in fetched_docs
    ]
    ctx = _compress_context(docs_serialized, question, max_chars=3500)

    # 3) ìƒì„±
    llm = _get_llm(temp=0.2, num_predict=_dynamic_num_predict(question, len(ctx)) if fast_mode else 200)
    prompt = USER_Q_TEMPLATE.format(question=question, context=ctx)
    msgs = [{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": prompt}]
    resp = llm.invoke(msgs)
    answer_text = (getattr(resp, "content", str(resp)) or "").strip()

    # 4) ì¶œì²˜ 1ê°œ ì„ íƒ(ë¹ ë¥¸ í‚¤ì›Œë“œ ì í•©ë„ ê¸°ë°˜)
    primary_meta = _select_primary_source_fast(question, docs_serialized, top_n=max(3, k))
    source_line = _primary_source_line(primary_meta)

    # 5) (ì„ íƒ) ì´ˆê²½ëŸ‰ ê²€ì¦ â€” ì‚¬ì‹¤ í‰ê·  0.6 ë¯¸ë§Œì´ë©´ ì£¼ì˜ ë¬¸êµ¬
    if strict_verify and not fast_mode:
        facts = _extract_facts_from_answer_cached(answer_text)
        if facts:
            to_eval_text = (primary_meta.get("title","") + "\n" + fetched_docs[0].page_content) if fetched_docs else ""
            with _f.ThreadPoolExecutor(max_workers=min(4, len(facts))) as ex:
                vals = list(ex.map(lambda f: _entailment_score_cached(f, to_eval_text), facts))
            avg_ent = sum(vals) / max(1, len(vals))
            if avg_ent < 0.6:
                answer_text += "\n\n(ì°¸ê³ : ê·¼ê±° ì¼ì¹˜ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ì¶”ê°€ ë¬¸ì„œë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.)"

    final = f"{answer_text}\n\nì›ë¬¸ ë§í¬:\n{source_line}"
    return final, fetched_docs

# ===================== UI =====================
st.title("ğŸ¤– ì°½ì¡° ê³¼í•™ ì±—ë´‡")

# LLM ì›Œë°(ì²« ì§ˆì˜ ë”œë ˆì´ ì™„í™”)
_ = _warm_llm_once()

with st.sidebar:
    st.subheader("âš™ï¸ ì„¤ì •")
    st.markdown("**ì„ë² ë”©/ìƒì„± ëª¨ë¸ì€ .env ë¡œ ì œì–´ë©ë‹ˆë‹¤.**")

    st.text_input("CSV ê²½ë¡œ", key="csv_path", value=CSV_DEFAULT)
    st.text_input("PERSIST_DIR", key="persist_dir", value=PERSIST_DIR)
    st.text_input("COLLECTION_NAME(ì„ íƒ)", key="collection_name", value=COLLECTION_NAME)

    st.markdown("---")
    top_k = st.slider("Top-K ë¬¸ì„œ", min_value=2, max_value=12, value=5, step=1)
    mode = st.selectbox("Retrieval Mode", ["Hybrid", "HyDE+Hybrid", "Lexical", "Dense"], index=0)
    dense_w = st.slider("Dense ê°€ì¤‘ì¹˜(í•˜ì´ë¸Œë¦¬ë“œ)", 0.0, 1.0, 0.6, 0.05)
    nb_win = st.slider("ì´ì›ƒ ì²­í¬ ë²”ìœ„", 0, 4, 1, 1)

    st.markdown("---")
    do_rebuild = st.button("ğŸ” ì¸ë±ìŠ¤ ì¬ë¹Œë“œ(FRESH)")

    st.markdown("---")
    fast_mode = st.checkbox("ğŸš€ Fast Mode (ìµœì†Œ í† í°Â·ì¦‰ì‹œ ì¶œì²˜ì„ ì •Â·HyDE ì•½í™”)", value=True)
    strict_verify = st.checkbox("ğŸ›¡ï¸ Strict Verify (ì‚¬ì‹¤-ë¬¸ì„œ ë¯¸ë‹ˆ ì—”í…Œì¼ë¨¼íŠ¸)", value=False,
                                help="ì •í™•ë„â†‘(ì•½ê°„ ëŠë ¤ì§). Fast Modeê°€ êº¼ì ¸ ìˆì„ ë•Œ íš¨ê³¼ì ")
    use_multiquery = st.checkbox("ğŸ” MultiQuery (HyDE+íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ)", value=True,
                                 help="ì •í™•ë„â†‘(ê²€ìƒ‰â†‘). Fast Modeê°€ ì¼œì ¸ ìˆìœ¼ë©´ ìë™ ì•½í™”")

    st.session_state.fast_mode = fast_mode
    st.session_state.strict_verify = strict_verify
    st.session_state.use_multiquery = use_multiquery

    # ì§„ë‹¨ íŒ¨ë„
    st.markdown("### ğŸ” í˜„ì¬ ì„¤ì • ì§„ë‹¨")
    st.code(f"CSV={st.session_state.get('csv_path', 'N/A')}\n"
            f"PERSIST_DIR={st.session_state.get('persist_dir', 'N/A')}\n"
            f"COLLECTION_NAME={st.session_state.get('collection_name', 'N/A')}\n"
            f"OLLAMA_BASE_URL={os.getenv('OLLAMA_BASE_URL')}\n"
            f"EMBED_MODEL={os.getenv('OLLAMA_EMBED_MODEL')}\n", language="bash")
    if st.button("CSV ì¡´ì¬/ì»¬ëŸ¼ ì ê²€"):
        try:
            _df = pd.read_csv(st.session_state.get('csv_path', ''), dtype=str).fillna("")
            st.success(f"CSV ë¡œë“œ OK, shape={_df.shape}")
            miss = {'url','title','content','references','further_refs'} - set(_df.columns)
            st.write("ëˆ„ë½ ì»¬ëŸ¼:", miss if miss else "ì—†ìŒ âœ…")
        except Exception as e:
            st.error(f"CSV ë¡œë“œ ì‹¤íŒ¨: {e}")

# ì„¸ì…˜ ìƒíƒœ
if "history" not in st.session_state:
    st.session_state.history = []

# ìŠ¤í† ì–´ ì¤€ë¹„
if "store" not in st.session_state or do_rebuild:
    try:
        store, d_used, cname_used = build_or_load_store(
            csv_path=st.session_state.csv_path,
            persist_dir=st.session_state.persist_dir,
            collection_name_env=st.session_state.collection_name,
            fresh=True if do_rebuild else False,
        )
        st.session_state.store = store
        st.session_state.persist_dir_used = d_used
        st.session_state.collection_used = cname_used
    except FileNotFoundError as e:
        st.error(f"CSV ê²½ë¡œ ë¬¸ì œ: {e}")
    except ValueError as e:
        st.error(f"CSV ì»¬ëŸ¼ ë¬¸ì œ: {e}")
    except RuntimeError as e:
        st.error(f"ì„ë² ë”© ëª¨ë¸ ë¬¸ì œ: {e}")
    except Exception as e:
        st.exception(e)

# ì±„íŒ… ì˜ì—­
st.markdown("### ğŸ’¬ ëŒ€í™”")
for role, content in st.session_state.history:
    with st.chat_message(role):
        st.markdown(content)

user_msg = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”â€¦ (ë¡œì»¬ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•©ë‹ˆë‹¤)")
if user_msg:
    st.session_state.history.append(("user", user_msg))
    with st.chat_message("user"):
        st.markdown(user_msg)

    with st.chat_message("assistant"):
        if "store" not in st.session_state:
            st.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ ì„¤ì • í™•ì¸ í›„ ì¬ë¹Œë“œí•˜ì„¸ìš”.")
            answer = "ìŠ¤í† ì–´ ë¯¸ì¤€ë¹„"
        else:
            with st.spinner("ê²€ìƒ‰ ë° ìƒì„± ì¤‘â€¦"):
                try:
                    answer, used_docs = rag_answer(
                        st.session_state.store,
                        question=user_msg,
                        k=top_k,
                        mode=mode,
                        dense_weight=dense_w,
                        neighbor_window=nb_win,
                        fast_mode=st.session_state.get("fast_mode", True),
                        use_multiquery=st.session_state.get("use_multiquery", True),
                        strict_verify=st.session_state.get("strict_verify", False),
                    )
                    st.markdown(answer)
                except Exception as e:
                    st.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
                    answer = f"ì˜¤ë¥˜: {e}"

    st.session_state.history.append(("assistant", answer))
