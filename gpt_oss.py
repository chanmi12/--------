import os
import re
import time
import hashlib
import random
import shutil
import math
from typing import List, Dict, Any, Tuple
import concurrent.futures as _f

import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import requests
import numpy as np

# --- Web UI / App ---
st.set_page_config(page_title="ë¡œì»¬ RAG ì±—ë´‡ (GPT-OSS + Chroma)", page_icon="ğŸ§ ", layout="wide")

# --- GPT-OSS ---
from gpt_oss import GPTModel
from gpt_oss.embeddings import GPTEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ===================== í™˜ê²½ ë¡œë“œ =====================
load_dotenv()

EMB_BACKEND = os.getenv("EMB_BACKEND", "gpt-oss").lower()
GPT_BASE_URL = os.getenv("GPT_BASE_URL", "http://localhost:8080")  # ë¡œì»¬ ì„œë²„ URLë¡œ ë³€ê²½
GPT_GEN_MODEL = os.getenv("GPT_GEN_MODEL", "gpt-oss-20b")  # ëª¨ë¸ ì´ë¦„ ìˆ˜ì • (ì˜ˆ: 20b ëª¨ë¸)
GPT_EMBED_MODEL = os.getenv("GPT_EMBED_MODEL", "gpt-oss-embeddings")

CSV_DEFAULT = os.getenv("CSV_DEFAULT", "test.csv")
PERSIST_DIR = os.getenv("PERSIST_DIR", "./chroma_creation")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "")

CHUNK_CHARS = int(os.getenv("CHUNK_CHARS", "1300"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "300"))
MAX_EMBED_CHARS = int(os.getenv("MAX_EMBED_CHARS", "3500"))
FRESH = os.getenv("FRESH", "true").lower() in ("1", "true", "yes", "y")

# ===================== ê³µìš© ìºì‹œ/ë¦¬ì†ŒìŠ¤ =====================
@st.cache_resource(show_spinner=False)
def _get_emb() -> GPTEmbeddings:
    return GPTEmbeddings(model=GPT_EMBED_MODEL, base_url=GPT_BASE_URL)

@st.cache_resource(show_spinner=False)
def _get_llm(temp: float = 0.2, num_predict: int = 200) -> GPTModel:
    return GPTModel(
        model_name=GPT_GEN_MODEL,
        temperature=temp,
        max_tokens=num_predict,
        api_url=GPT_BASE_URL
    )

# ===================== ìœ í‹¸ =====================
@st.cache_data(show_spinner=False)
def _cached_read_csv(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path, dtype=str).fillna("")
    return df

def df_fingerprint(df: pd.DataFrame) -> str:
    parts = [(row.get("title", "") or "") + (row.get("content", "") or "") for _, row in df.iterrows()]
    return hashlib.sha1("|".join(parts).encode("utf-8")).hexdigest()

def persist_path(persist_dir: str, fp: str) -> Tuple[str, str]:
    d = os.path.join(persist_dir, f"chroma_{fp[:12]}")
    return d, f"creation_{fp[:12]}"

def load_csv(csv_path: str) -> pd.DataFrame:
    csv_path = csv_path if os.path.isabs(csv_path) else os.path.join(os.getcwd(), csv_path)
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
    df = _cached_read_csv(csv_path)
    need_cols = {"url", "title", "content", "references", "further_refs"}
    missing = need_cols - set(df.columns)
    if missing:
        raise ValueError(f"CSVì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing}")
    return df

def need_force_fetch(url: str) -> bool:
    try:
        host = re.sub(r"^https?://", "", url).split("/")[0]
        return any(host.endswith(dom) for dom in FORCE_FETCH_DOMAINS)
    except Exception:
        return False

def _smart_select_main(soup: BeautifulSoup):
    for css in [
        "article", ".fr-view", ".rd-content", ".board_view", ".boardView", ".content",
        "#content", "#article", "#view", ".editor_content", ".xe_content", ".se-component",
    ]:
        node = soup.select_one(css)
        if node and node.get_text(strip=True):
            return node
    return soup.body or soup

@st.cache_data(show_spinner=False)
def fetch_url_text(url: str, timeout: int = 12, max_len: int = 25000, retries: int = 2) -> str:
    if not url or not re.match(r"^https?://", url):
        return ""
    for attempt in range(retries + 1):
        try:
            r = requests.get(url, headers={"User-Agent": "Mozilla/5.0 (CreationKR/1.0)"}, timeout=timeout)
            r.raise_for_status()
            if not r.encoding or r.encoding.lower() in ("iso-8859-1", "ascii"):
                r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")
            for tag in soup(["script", "style", "nav", "footer", "header", "aside", "form"]):
                tag.decompose()
            main = _smart_select_main(soup)
            text = (main or soup).get_text("\n")
            lines = [re.sub(r"\s+", " ", ln).strip() for ln in text.splitlines()]
            text = "\n".join([ln for ln in lines if ln])[:max_len]
            if len(text) < 150 and attempt < retries:
                time.sleep(0.2)
                continue
            return text
        except Exception:
            time.sleep(0.2)
    return ""

def safe_truncate(s: str, max_chars: int) -> str:
    if len(s) <= max_chars:
        return s
    cut = s[:max_chars]
    last = max(cut.rfind("\n"), cut.rfind(". "), cut.rfind("ã€‚"), cut.rfind("! "), cut.rfind("? "))
    if last >= max_chars * 0.7:
        return cut[:last].rstrip()
    return cut.rstrip()

@st.cache_data(show_spinner=False)
def _split_base_text(base_text: str) -> List[str]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_CHARS,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", " ", ""],
    )
    return splitter.split_text(base_text)

# ê¸°íƒ€ í•„ìš”í•œ í•¨ìˆ˜ë“¤ (ê²€ìƒ‰, ì¸ë±ìŠ¤ êµ¬ì¶• ë“±)ë“¤ì€ ìœ„ì™€ ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ GPT-OSSì™€ í˜¸í™˜ë˜ê²Œ ìˆ˜ì • ê°€ëŠ¥í•©ë‹ˆë‹¤.

# ===================== í”„ë¡¬í”„íŠ¸ & RAG ì²´ì¸ =====================
SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í•œêµ­ì–´ ì—°êµ¬ ë³´ì¡°ìì…ë‹ˆë‹¤. "
    "ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì§€ì‹ì˜ ê·¼ê±°ë¡œ ì‚¼ë˜, ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ì§€ ë§ê³  ë°˜ë“œì‹œ ì¬êµ¬ì„±í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•˜ì„¸ìš”. "
    "í‘œí˜„ ë°©ì‹, ë¬¸ì¥ êµ¬ì¡°, ì„¤ëª… íë¦„ì„ ë°”ê¿”ì„œ ìƒˆë¡­ê²Œ ì„œìˆ í•´ì•¼ í•©ë‹ˆë‹¤. "
    "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì‚¬ì‹¤ì€ ë§í•˜ì§€ ë§ê³ , ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”."
)


USER_Q_TEMPLATE = (
    "ì§ˆë¬¸: {question}\n\n"
    "[ì»¨í…ìŠ¤íŠ¸]\n{context}\n\n"
    "ìš”êµ¬ì‚¬í•­:\n"
    "- ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ë¬¸ì„œì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”."
    "- ë³¸ë¬¸ì—ëŠ” ë§í¬/ì¶œì²˜ë¥¼ ì“°ì§€ ë§ˆì„¸ìš”.\n"
    "- ì˜ëª»ëœ ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.\n"
    "- ë§ì¶¤ë²•ì€ ì§€í‚¤ì„¸ìš”.\n"
)

# ===================== UI =====================
st.title("ğŸ¤– ì°½ì¡° ê³¼í•™ ì±—ë´‡")

# GPT-OSS ë¡œì»¬ ì„œë²„ ì›Œë°(ì²« ì§ˆì˜ ë”œë ˆì´ ì™„í™”)
llm = _get_llm()
_ = llm.generate("Hello, how are you?")

# ì„¸ì…˜ ìƒíƒœ
if "history" not in st.session_state:
    st.session_state.history = []

# ì±„íŒ… ì˜ì—­
st.markdown("### ğŸ’¬ ëŒ€í™”")
for role, content in st.session_state.history:
    with st.chat_message(role):
        st.markdown(content)

user_msg = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”â€¦ (ë¡œì»¬ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•©ë‹ˆë‹¤)")
if user_msg:
    st.session_state.history.append(("user", user_msg))
    with st.chat_message("user"):
        st.markdown(user_msg)

    with st.chat_message("assistant"):
        try:
            answer = llm.generate(user_msg)
            st.markdown(answer)
        except Exception as e:
            st.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")

    st.session_state.history.append(("assistant", answer))
